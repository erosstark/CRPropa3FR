{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "284f279d-7ff5-4926-84c7-8952ab8718d5",
      "metadata": {
        "id": "284f279d-7ff5-4926-84c7-8952ab8718d5"
      },
      "source": [
        "# Aula 04: Classificação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac64c6e9-ee5c-4741-b0a2-ae8a5c6d7aa5",
      "metadata": {
        "id": "ac64c6e9-ee5c-4741-b0a2-ae8a5c6d7aa5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29eb6d53-6463-4843-8cbd-303f7ce48e1a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "29eb6d53-6463-4843-8cbd-303f7ce48e1a"
      },
      "source": [
        "# Sumário"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61bae59-a732-4328-a254-ea7624f86341",
      "metadata": {
        "id": "b61bae59-a732-4328-a254-ea7624f86341"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "1. Implementar o método de regressão logística usado em problemas de classificação;\n",
        "2. Implementar uma extensão do método fazendo regularização com a norma $L_2$ (i.e. _Ridge regression_).\n",
        "\n",
        "## Exercícios\n",
        "\n",
        "### Parte 1: Regressão Logística\n",
        "- Exercício 1: Preparação dos dados\n",
        "- Exercício 2: Regressão logística - método do gradiente\n",
        "- Exercício 3: Regressão logística - scikit-learn\n",
        "\n",
        "### Parte 2: Regularização\n",
        "- Exercício 4: Preparação dos dados\n",
        "- Exercício 5: Regressão logística com regularização - método do gradiente\n",
        "- Exercício 6: Regressão logística com regularização - scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "382a8963-2456-4acc-aa9f-736c3b4e74c0",
      "metadata": {
        "id": "382a8963-2456-4acc-aa9f-736c3b4e74c0"
      },
      "source": [
        "# Parte 1: Regressão logística"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0733a177-360c-40a3-b120-0b328ecbc637",
      "metadata": {
        "id": "0733a177-360c-40a3-b120-0b328ecbc637",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Revisão teórica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb6109e2-d1f9-47c8-b771-a369ac84aaa9",
      "metadata": {
        "id": "bb6109e2-d1f9-47c8-b771-a369ac84aaa9"
      },
      "source": [
        "### Motivação\n",
        "Muitos problemas práticos de aprendizado de máquina envolvem **classificação binária**, isto é, prever uma entre duas categorias (por exemplo, **sim/não**, **0/1**, **sucesso/falha**). Nesses casos, não faz sentido usar regressão linear convencional, pois ela produz valores contínuos não limitados, podendo extrapolar além do intervalo [0,1]. Para problemas de classificação, desejamos em vez disso estimar diretamente a **probabilidade** de um evento binário ocorrer, dado um vetor de atributos de entrada.\n",
        "\n",
        "### Modelo de Regressão Logística\n",
        "A **regressão logística** modela diretamente a probabilidade de um evento binário e permite definir um critério para obter uma fronteira de decisão bem definida. Para obter saídas probabilísticas, aplicamos a função sigmoide a uma combinação linear:\n",
        "\n",
        "$$\n",
        "f_{\\mathbf{w},b}(\\mathbf{x}) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)\\,, \\tag{1}\n",
        "$$\n",
        "$$\n",
        "g(z) = \\frac{1}{\\,1 + e^{-z}\\,}\\,. \\tag{2}\n",
        "$$\n",
        "A sigmoide é monotônica e mapeia $\\mathbb{R}$ para $(0,1)$; logo, $f_{\\mathbf{w},b}(\\mathbf{x})$ é sempre interpretável como uma probabilidade. O hiperplano $\\mathbf{w}\\cdot\\mathbf{x}+b=0$ corresponde a $f_{\\mathbf{w},b}=0.5$.\n",
        "\n",
        "### Função de Custo\n",
        "Ajustamos $(\\mathbf{w}, b)$ minimizando a média da perda logarítmica binária, apropriada para alvos Bernoulli:\n",
        "$$\n",
        "J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big[ \\text{loss}\\!\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) \\Big] \\tag{3}\n",
        "$$\n",
        "$$\n",
        "\\text{loss}\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) = -\\,y^{(i)} \\log\\!\\Big(f_{\\mathbf{w},b}\\big( \\mathbf{x}^{(i)} \\big)\\Big)\\;-\\;\\big(1 - y^{(i)}\\big)\\,\\log\\!\\Big(1 - f_{\\mathbf{w},b}\\big( \\mathbf{x}^{(i)} \\big)\\Big)\\,. \\tag{4}\n",
        "$$\n",
        "\n",
        "Se $y^{(i)}=1$, o segundo termo se anula; já caso $y^{(i)}=0$, o primeiro termo se anula.\n",
        "\n",
        "### Gradiente da Função de Custo\n",
        "As derivadas parciais podem ser obtidos com a regra da cadeia e possuem a seguinte forma:\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,x_{j}^{(i)}\\,, \\quad \\text{para cada } j=0,1,\\dots,n-1\\,. \\tag{5a}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,, \\tag{5b}\n",
        "$$\n",
        "As expressões acima revelam que o gradiente do custo em relação a cada peso $w_j$ é proporcional à média dos erros *$(\\text{previsão} - \\text{valor real})$* multiplicados pelo respectivo atributo $x_j$. Já o gradiente em relação a $b$ (termo de bias) é a média simples dos erros $(f_{\\mathbf{w},b}(\\mathbf{x}) - y)$.\n",
        "\n",
        "**Embora a forma lembre a da regressão linear, aqui $f_{\\mathbf{w},b}$ é sigmoidal.**\n",
        "\n",
        "### Ajuste de Parâmetros com o Método do Gradiente\n",
        "Para minimizar $J(\\mathbf{w},b)$ de forma iterativa, utilizamos o **método do gradiente**. Inicializamos os pesos $\\mathbf{w}$ e bias $b$ (por exemplo, com zeros ou valores pequenos aleatórios) e então atualizamos os parâmetros repetidamente na direção oposta ao gradiente, com um passo proporcional à **taxa de aprendizado** $\\alpha > 0$. A regra de atualização em cada iteração é dada por:\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\text{Repeat } \\{ \\\\\n",
        "& \\quad b \\;:=\\; b \\;-\\; \\alpha \\;\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}, \\tag{6a} \\\\\n",
        "& \\quad w_j \\;:=\\; w_j \\;-\\; \\alpha \\;\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}, \\quad \\text{para } j = 0,\\ldots,n-1. \\tag{6b}\\\\\n",
        "& \\}\n",
        "\\end{align*}\n",
        "$$\n",
        "Esse procedimento é repetido iterativamente até que $J(\\mathbf{w},b)$ convirja para um mínimo (ou até atingir um número máximo de épocas).\n",
        "\n",
        "### Interpretação Probabilística e Regra de Decisão\n",
        "Um dos principais atrativos da regressão logística é a sua **interpretação probabilística**. A saída do modelo $f_{\\mathbf{w},b}(\\mathbf{x})$ pode ser entendida como a probabilidade estimada de $y=1$ dado o vetor de atributos $\\mathbf{x}$ – isto é, $f_{\\mathbf{w},b}(\\mathbf{x}) \\approx P(y=1 \\mid \\mathbf{x})$. Por exemplo, se $f_{\\mathbf{w},b}(\\mathbf{x}) = 0.85$, o modelo estima **85% de chance** de o exemplo pertença à classe positiva ($y=1$). Essa interpretação é coerente com a forma funcional do modelo: estamos basicamente ajustando uma distribuição de Bernoulli, cuja média (probabilidade de sucesso) depende de $\\mathbf{x}$ através da função sigmoide.\n",
        "\n",
        "Para utilizar o modelo em tarefas de classificação, é preciso converter a probabilidade prevista em um **rótulo binário** $\\hat{y}$. A regra de decisão mais comum é aplicar um **limiar de 0.5**:\n",
        "\n",
        "- Se $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) \\geq 0.5$, então classificamos $\\hat{y}^{(i)} = 1$ (classe positiva).  \n",
        "- Caso contrário ($f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) < 0.5$), classificamos $\\hat{y}^{(i)} = 0$ (classe negativa).\n",
        "\n",
        "### Resumo\n",
        "A regressão logística combina um modelo linear com a sigmoide para produzir probabilidades em $(0,1)$, otimizadas via perda logarítmica e gradiente descendente, resultando em um classificador binário simples, interpretável e eficaz.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "doCpahQBbxpG",
      "metadata": {
        "id": "doCpahQBbxpG",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 1: Preparação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N6Oki-S1ncc1",
      "metadata": {
        "id": "N6Oki-S1ncc1"
      },
      "source": [
        "Como no notebook da Aula 3, vamos usar dados simplificados. Estes dados descrevem o resultado do processo de admissão em uma universidade americana fictícia, em função das notas obtidas em dois exames distintos. Cada linha corresponde a um estudante e contém 3 colunas: duas colunas com notas numéricas dos exames 1 e 2, e uma terceira coluna com uma variável binária (0/1), onde 1 significa que o estudante foi aceito."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd16628-3c51-4ff1-b5d7-e57fdd259c1d",
      "metadata": {
        "id": "1dd16628-3c51-4ff1-b5d7-e57fdd259c1d"
      },
      "source": [
        "### Criando o conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fdd3ec27-27cb-4f70-843e-0b5c4f5914df",
      "metadata": {
        "id": "fdd3ec27-27cb-4f70-843e-0b5c4f5914df"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./data\n",
        "!touch ./data/admissions.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4b1191f7-aa83-4816-a076-a24d92fbd4fd",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "4b1191f7-aa83-4816-a076-a24d92fbd4fd"
      },
      "outputs": [],
      "source": [
        "admissions_data = \"\"\"34.62365962451697,78.0246928153624,0\n",
        "30.28671076822607,43.89499752400101,0\n",
        "35.84740876993872,72.90219802708364,0\n",
        "60.18259938620976,86.30855209546826,1\n",
        "79.0327360507101,75.3443764369103,1\n",
        "45.08327747668339,56.3163717815305,0\n",
        "61.10666453684766,96.51142588489624,1\n",
        "75.02474556738889,46.55401354116538,1\n",
        "76.09878670226257,87.42056971926803,1\n",
        "84.43281996120035,43.53339331072109,1\n",
        "95.86155507093572,38.22527805795094,0\n",
        "75.01365838958247,30.60326323428011,0\n",
        "82.30705337399482,76.48196330235604,1\n",
        "69.36458875970939,97.71869196188608,1\n",
        "39.53833914367223,76.03681085115882,0\n",
        "53.9710521485623,89.20735013750205,1\n",
        "69.07014406283025,52.74046973016765,1\n",
        "67.94685547711617,46.67857410673128,0\n",
        "70.66150955499435,92.92713789364831,1\n",
        "76.97878372747498,47.57596364975532,1\n",
        "67.37202754570876,42.83843832029179,0\n",
        "89.67677575072079,65.79936592745237,1\n",
        "50.534788289883,48.85581152764205,0\n",
        "34.21206097786789,44.20952859866288,0\n",
        "77.9240914545704,68.9723599933059,1\n",
        "62.27101367004632,69.95445795447587,1\n",
        "80.1901807509566,44.82162893218353,1\n",
        "93.114388797442,38.80067033713209,0\n",
        "61.83020602312595,50.25610789244621,0\n",
        "38.78580379679423,64.99568095539578,0\n",
        "61.379289447425,72.80788731317097,1\n",
        "85.40451939411645,57.05198397627122,1\n",
        "52.10797973193984,63.12762376881715,0\n",
        "52.04540476831827,69.43286012045222,1\n",
        "40.23689373545111,71.16774802184875,0\n",
        "54.63510555424817,52.21388588061123,0\n",
        "33.91550010906887,98.86943574220611,0\n",
        "64.17698887494485,80.90806058670817,1\n",
        "74.78925295941542,41.57341522824434,0\n",
        "34.1836400264419,75.2377203360134,0\n",
        "83.90239366249155,56.30804621605327,1\n",
        "51.54772026906181,46.85629026349976,0\n",
        "94.44336776917852,65.56892160559052,1\n",
        "82.36875375713919,40.61825515970618,0\n",
        "51.04775177128865,45.82270145776001,0\n",
        "62.22267576120188,52.06099194836679,0\n",
        "77.19303492601364,70.45820000180959,1\n",
        "97.77159928000232,86.7278223300282,1\n",
        "62.07306379667647,96.76882412413983,1\n",
        "91.56497449807442,88.69629254546599,1\n",
        "79.94481794066932,74.16311935043758,1\n",
        "99.2725269292572,60.99903099844988,1\n",
        "90.54671411399852,43.39060180650027,1\n",
        "34.52451385320009,60.39634245837173,0\n",
        "50.2864961189907,49.80453881323059,0\n",
        "49.58667721632031,59.80895099453265,0\n",
        "97.64563396007767,68.86157272420604,1\n",
        "32.57720016809309,95.59854761387875,0\n",
        "74.24869136721598,69.82457122657193,1\n",
        "71.79646205863379,78.45356224515052,1\n",
        "75.3956114656803,85.75993667331619,1\n",
        "35.28611281526193,47.02051394723416,0\n",
        "56.25381749711624,39.26147251058019,0\n",
        "30.05882244669796,49.59297386723685,0\n",
        "44.66826172480893,66.45008614558913,0\n",
        "66.56089447242954,41.09209807936973,0\n",
        "40.45755098375164,97.53518548909936,1\n",
        "49.07256321908844,51.88321182073966,0\n",
        "80.27957401466998,92.11606081344084,1\n",
        "66.74671856944039,60.99139402740988,1\n",
        "32.72283304060323,43.30717306430063,0\n",
        "64.0393204150601,78.03168802018232,1\n",
        "72.34649422579923,96.22759296761404,1\n",
        "60.45788573918959,73.09499809758037,1\n",
        "58.84095621726802,75.85844831279042,1\n",
        "99.82785779692128,72.36925193383885,1\n",
        "47.26426910848174,88.47586499559782,1\n",
        "50.45815980285988,75.80985952982456,1\n",
        "60.45555629271532,42.50840943572217,0\n",
        "82.22666157785568,42.71987853716458,0\n",
        "88.9138964166533,69.80378889835472,1\n",
        "94.83450672430196,45.69430680250754,1\n",
        "67.31925746917527,66.58935317747915,1\n",
        "57.23870631569862,59.51428198012956,1\n",
        "80.36675600171273,90.96014789746954,1\n",
        "68.46852178591112,85.59430710452014,1\n",
        "42.0754545384731,78.84478600148043,0\n",
        "75.47770200533905,90.42453899753964,1\n",
        "78.63542434898018,96.64742716885644,1\n",
        "52.34800398794107,60.76950525602592,0\n",
        "94.09433112516793,77.15910509073893,1\n",
        "90.44855097096364,87.50879176484702,1\n",
        "55.48216114069585,35.57070347228866,0\n",
        "74.49269241843041,84.84513684930135,1\n",
        "89.84580670720979,45.35828361091658,1\n",
        "83.48916274498238,48.38028579728175,1\n",
        "42.2617008099817,87.10385094025457,1\n",
        "99.31500880510394,68.77540947206617,1\n",
        "55.34001756003703,64.9319380069486,1\n",
        "74.77589300092767,89.52981289513276,1\"\"\"\n",
        "\n",
        "with open('./data/admissions.csv', 'w') as f:\n",
        "    f.write(admissions_data)\n",
        "\n",
        "# Importando dados\n",
        "\n",
        "X_features = ['Exam 1 score', 'Exam 2 score']\n",
        "y_name = ['Admitted']\n",
        "data = pd.read_csv(\"./data/admissions.csv\", header=None, names=X_features + y_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2584caaa-9cb4-4e1c-8031-021f012604b8",
      "metadata": {
        "id": "2584caaa-9cb4-4e1c-8031-021f012604b8"
      },
      "source": [
        "### 1.a) Crie a matriz de treino $X_{\\rm train}$, o vetor de variáveis alvo $y_{\\rm train}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "042f3e65-a91d-491b-b924-42e9dcfd364b",
      "metadata": {
        "id": "042f3e65-a91d-491b-b924-42e9dcfd364b"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data[X_features].values\n",
        "y = data[y_name].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fbedc18-183a-45c8-83cb-812ddab7dcfd",
      "metadata": {
        "id": "4fbedc18-183a-45c8-83cb-812ddab7dcfd"
      },
      "source": [
        "### 1b) Explore (rapidamente) os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8757a203-2422-440e-96ee-8bcde5deca90",
      "metadata": {
        "id": "8757a203-2422-440e-96ee-8bcde5deca90"
      },
      "source": [
        "**Inspecione a tabela com funcionalidade da biblioteca pandas e faça um scatterplot dos dados, usando a coluna \"Admitted\" como cor do marcador.**\n",
        "\n",
        "_Obs: não gaste muito tempo neste exercício. Por outro lado, é sempre importante inspecionar rapidamente os dados._\n",
        "\n",
        "\n",
        "> Besides `display()`, you can use several other methods to inspect a pandas DataFrame:\n",
        ">\n",
        "> - `.head()`: Shows the first 5 rows (or a specified number) of the DataFrame. Useful for a quick look at the data structure.\n",
        "> - `.info()`: Prints a concise summary of the DataFrame, including the index dtype > and column dtypes, non-null values and memory usage.\n",
        "> - `.describe()`: Generates descriptive statistics of the DataFrame's numerical > columns, such as count, mean, standard deviation, minimum, and maximum.\n",
        "> - `.shape`: Returns a tuple representing the dimensionality of the DataFrame (rows, > columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "885cf223-a8db-4a21-aa5e-3efca5a68760",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "885cf223-a8db-4a21-aa5e-3efca5a68760",
        "outputId": "01b87dad-7e57-450c-e28f-ef60068ff429"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pandas' has no attribute 'info'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3168340167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ---- INSIRA SEU CÓDIGO AQUI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'info'"
          ]
        }
      ],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc1b003-bf09-4c18-8f22-1b12c8d12ca0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "fbc1b003-bf09-4c18-8f22-1b12c8d12ca0"
      },
      "source": [
        "## Exercício 2: Regressão logística - Método do gradiente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab34091-3671-4731-a482-3bfbdff5c282",
      "metadata": {
        "id": "5ab34091-3671-4731-a482-3bfbdff5c282"
      },
      "source": [
        "O objetivo deste exercício é implementar o método do gradiente para regressão logística."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa515c6e-9948-48b7-8609-c6e2d12f2695",
      "metadata": {
        "id": "aa515c6e-9948-48b7-8609-c6e2d12f2695"
      },
      "source": [
        "### 2.a) Implemente as funções do método do gradiente em regressão logística"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b71d8ca-73bf-4077-a6db-8478a2baa498",
      "metadata": {
        "id": "1b71d8ca-73bf-4077-a6db-8478a2baa498"
      },
      "source": [
        "Será necessário definir as seguintes funções:\n",
        "\n",
        "- `sigmoid` define a função sigmóide\n",
        "- `compute_cost` implementa a equação (3) acima;\n",
        "- `compute_gradient` implementa as equações (5a) e (5b) acima;\n",
        "- `gradient_descent` implementa o método do gradiente segundo as equações (6a) e (6b) acima;\n",
        "- `predict` implementa o critério de corte que define a etiqueta prevista pelo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b5faae43-3770-4d3c-9465-98c9a6bacb83",
      "metadata": {
        "id": "b5faae43-3770-4d3c-9465-98c9a6bacb83"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z (ndarray): A scalar, numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g (ndarray): sigmoid(z), with the same shape as z\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "\n",
        "    # ----\n",
        "\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "\n",
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost $J(\\vec{w}, b)$ for a logistic regression model.\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m, n)): Data, m examples with n features each\n",
        "      y (ndarray (m,))  : target values\n",
        "      w (ndarray (n,))  : model weight parameters\n",
        "      b (scalar)        : model bias parameter\n",
        "\n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    cost = 0\n",
        "    w_init = w\n",
        "    b_init = b\n",
        "    m = X.shape[0]\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i], w) + b)\n",
        "        cost += -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
        "\n",
        "    cost = cost / m\n",
        "    # ----\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression\n",
        "    Args:\n",
        "      X (ndarray (m, n)): Data, m examples with n features each\n",
        "      y (ndarray (m,))  : target values\n",
        "      w (ndarray (n,))  : model weight parameters\n",
        "      b (scalar)        : model bias parameter\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0.\n",
        "    m = X.shape[0]\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i], w) + b)\n",
        "        dj_dw += (f_wb_i - y[i])*X[i]\n",
        "        dj_db += f_wb_i - y[i]\n",
        "\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "    # ----\n",
        "\n",
        "    return dj_dw, dj_db\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m, n)) : Data, m examples with n features each\n",
        "      y (ndarray (m,))   : target values\n",
        "      w_in (ndarray (n,)): initial model weight parameters\n",
        "      b_in (scalar)      : initial model bias parameter\n",
        "      alpha (float)      : Learning rate\n",
        "      num_iters (int)    : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,))                   : Updated values of parameters\n",
        "      b (scalar)                         : Updated value of parameter\n",
        "      J_history (ndarray (num_iters,))   : History of cost values\n",
        "      w_history (ndarray (num_iters, n)) : History of parameters w\n",
        "      b_history (ndarray (num_iters,))   : History of parameters b\n",
        "      \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "    b_history = []\n",
        "    w = w_in\n",
        "    b = b_in\n",
        "    for i in range(num_iters):\n",
        "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "        J_history.append(compute_cost(X, y, w, b))\n",
        "        w_history.append(w)\n",
        "        b_history.append(b)\n",
        "\n",
        "\n",
        "\n",
        "    # ----\n",
        "\n",
        "    return w, b, J_history, w_history, b_history\n",
        "\n",
        "\n",
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    Predict whether the label is 0 or 1 using learned logistic\n",
        "    regression parameters w\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "\n",
        "    Returns:\n",
        "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "\n",
        "    m, n = X.shape\n",
        "    p = np.zeros(m)\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i], w) + b)\n",
        "        p[i] = 1 if f_wb_i >= 0.5 else 0\n",
        "\n",
        "    # ----\n",
        "\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbfee876-fa88-426d-82f3-235ebfa72469",
      "metadata": {
        "id": "dbfee876-fa88-426d-82f3-235ebfa72469"
      },
      "source": [
        "### 2.b) Treine o modelo, faça previsões e inspecione graficamente os resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b08e02-7fb3-4447-8272-9fc74a4839a9",
      "metadata": {
        "id": "18b08e02-7fb3-4447-8272-9fc74a4839a9"
      },
      "source": [
        "Faça a regressão logística nos dados e verifique graficamente que o comportamento da função de custo está como o esperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3b1bc73b-c8cf-4326-9572-42de644c18b9",
      "metadata": {
        "id": "3b1bc73b-c8cf-4326-9572-42de644c18b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "a1f73285-d0a2-4793-a37a-cbe2a45fb880"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 32.4 s, sys: 91.4 ms, total: 32.5 s\n",
            "Wall time: 32.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "\n",
        "w = np.zeros(X_train.shape[1])\n",
        "b = 0\n",
        "alpha = 0.0001\n",
        "num_iters = 10000\n",
        "\n",
        "w, b, J_history, w_history, b_history = gradient_descent(X_train, y_train, w, b, alpha, num_iters)\n",
        "\n",
        "# ----\n",
        "\n",
        "plt.plot(J_history)\n",
        "plt.show()\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e26f8e5-7556-409c-bb0c-4fa03acf282e",
      "metadata": {
        "id": "9e26f8e5-7556-409c-bb0c-4fa03acf282e",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 3: Regressão logística - Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64ebd10-8431-467a-b0e9-168bbaf031b8",
      "metadata": {
        "id": "c64ebd10-8431-467a-b0e9-168bbaf031b8"
      },
      "source": [
        "O objetivo deste exercício é obter os memos resultados do exercício acima usando a funcionalidade já existente no scikit-learn (`LogisticRegression`). Calcule também a acurácia usando a função `score` e faça um `plt.scatter` com código de cor correspondendo a sucesso/fracasso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc3c9ce-279a-4145-82ab-ee34b86847b5",
      "metadata": {
        "id": "fbc3c9ce-279a-4145-82ab-ee34b86847b5"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed13176-40e6-4483-8cd5-93b56b522bdc",
      "metadata": {
        "id": "2ed13176-40e6-4483-8cd5-93b56b522bdc"
      },
      "source": [
        "# Parte 2: Regularização"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c6f9a34-696f-48b8-8c9e-626aa26dac1e",
      "metadata": {
        "id": "8c6f9a34-696f-48b8-8c9e-626aa26dac1e",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Revisão teórica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c352d18b-0a53-4a2d-ae7e-edcb9606de1a",
      "metadata": {
        "id": "c352d18b-0a53-4a2d-ae7e-edcb9606de1a"
      },
      "source": [
        "### Motivação\n",
        "Modelos de classificação tendem a **sobreajustar** quando há muitas *features* ou correlações fortes. A **regularização** introduz um termo de penalização no objetivo para controlar a complexidade do modelo, reduzindo variância e melhorando a generalização, sem alterar a forma do classificador logístico.\n",
        "\n",
        "### Modelo de Regressão Logística (inalterado)\n",
        "A predição probabilística continua sendo dada pela composição de uma combinação linear com a sigmoide:\n",
        "$$\n",
        "f_{\\mathbf{w},b}(\\mathbf{x}) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)\\,, \\tag{1}\n",
        "$$\n",
        "$$\n",
        "g(z) = \\frac{1}{\\,1 + e^{-z}\\,}\\,. \\tag{2}\n",
        "$$\n",
        "\n",
        "### Função de Custo Regularizada\n",
        "A regularização modifica a função de custo adicionando uma penalização aos pesos. Em **L2 (Ridge)**, penalizamos a norma quadrática dos pesos (não se regulariza o viés $b$):\n",
        "$$\n",
        "J_\\lambda(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big[ \\text{loss}\\!\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) \\Big] \\;+\\; \\frac{\\lambda}{2m}\\,\\|\\mathbf{w}\\|_2^2\\,. \\tag{3}\n",
        "$$\n",
        "Alternativamente, em **L1 (Lasso)**:\n",
        "$$\n",
        "J_\\lambda^{(L1)}(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big[ \\text{loss}\\!\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) \\Big] \\;+\\; \\frac{\\lambda}{m}\\,\\|\\mathbf{w}\\|_1\\,. \\tag{4}\n",
        "$$\n",
        "Aqui, a perda logarítmica binária permanece:\n",
        "$$\n",
        "\\text{loss}\\big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),\\, y^{(i)}\\big) = -\\,y^{(i)} \\log\\!\\Big(f_{\\mathbf{w},b}\\big( \\mathbf{x}^{(i)} \\big)\\Big)\\;-\\;\\big(1 - y^{(i)}\\big)\\,\\log\\!\\Big(1 - f_{\\mathbf{w},b}\\big( \\mathbf{x}^{(i)} \\big)\\Big)\\,. \\tag{5}\n",
        "$$\n",
        "\n",
        "### Gradientes (L2) e Subgradientes (L1)\n",
        "Para **L2**, os gradientes tornam-se:\n",
        "$$\n",
        "\\frac{\\partial J_\\lambda(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,x_{j}^{(i)} \\;+\\; \\frac{\\lambda}{m}\\,w_j\\,, \\quad j=0,\\dots,n-1\\,, \\tag{6a}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J_\\lambda(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,. \\tag{6b}\n",
        "$$\n",
        "Para **L1**, usa-se **subgradiente** nos pesos:\n",
        "$$\n",
        "\\partial_{w_j} J_\\lambda^{(L1)}(\\mathbf{w},b) \\;\\ni\\; \\frac{1}{m}\\sum_{i=0}^{m-1} \\Big(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\Big)\\,x_{j}^{(i)} \\;+\\; \\frac{\\lambda}{m}\\,\\operatorname{sgn}(w_j)\\,, \\tag{7}\n",
        "$$\n",
        "onde $\\operatorname{sgn}(0)\\in[-1,1]$. Em ambos os casos, o viés $b$ não é penalizado e mantém o gradiente da regressão logística sem regularização.\n",
        "\n",
        "### Ajuste de Parâmetros por Gradiente\n",
        "O esquema de atualização preserva a forma, acrescentando apenas o termo de penalização no passo de $\\mathbf{w}$ (L2) ou usando subgradientes (L1):\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\text{Repeat } \\{ \\\\\n",
        "& \\quad b \\;:=\\; b \\;-\\; \\alpha \\;\\frac{\\partial J_\\lambda(\\mathbf{w},b)}{\\partial b}, \\tag{8a} \\\\\n",
        "& \\quad w_j \\;:=\\; w_j \\;-\\; \\alpha \\;\\frac{\\partial J_\\lambda(\\mathbf{w},b)}{\\partial w_j}, \\quad j = 0,\\ldots,n-1. \\tag{8b}\\\\\n",
        "& \\}\n",
        "\\end{align*}\n",
        "$$\n",
        "A escolha de $\\lambda>0$ controla o compromisso viés–variância: maiores valores induzem pesos menores e fronteiras mais “suaves”.\n",
        "\n",
        "### Interpretação e Regra de Decisão\n",
        "A interpretação probabilística permanece: $f_{\\mathbf{w},b}(\\mathbf{x}) \\approx P(y=1\\mid \\mathbf{x})$. A regra de decisão é a mesma:\n",
        "- Se $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) \\geq 0.5$, então $\\hat{y}^{(i)}=1$; caso contrário, $\\hat{y}^{(i)}=0$.\n",
        "A regularização tende a reduzir a variância do classificador e pode melhorar desempenho fora da amostra, especialmente em cenários com muitas *features* ou colinearidade.\n",
        "\n",
        "### Resumo\n",
        "A regularização adiciona uma penalização aos pesos na função de custo: **L2** encolhe coeficientes de modo suave e estabiliza o problema; **L1** promove esparsidade, realizando seleção de variáveis. Em ambos os casos, o modelo logístico e sua interpretação probabilística permanecem, enquanto a complexidade é controlada via $\\lambda$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55442794-c671-4fa6-9cae-cde09fd93499",
      "metadata": {
        "id": "55442794-c671-4fa6-9cae-cde09fd93499",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 4: Preparação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ec864a-27f1-4b6c-ab56-5da68d4330a7",
      "metadata": {
        "id": "07ec864a-27f1-4b6c-ab56-5da68d4330a7"
      },
      "source": [
        "Para os próximos dois exercícios, vamos usar um conjunto de dados diferentes. Uma fábrica produz microchips e o objetivo do modelo de regressão logística é prever se um dado microchip passará pelos testes de qualidade (QA, de _quality assurance_)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739a090e-895b-4737-83e7-911aa407d1ae",
      "metadata": {
        "id": "739a090e-895b-4737-83e7-911aa407d1ae"
      },
      "source": [
        "### Criando o conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ffc328-f0db-4012-bff7-87111e238ad0",
      "metadata": {
        "id": "c7ffc328-f0db-4012-bff7-87111e238ad0"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./data\n",
        "!touch ./data/microchips.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a59577c-929d-47ff-990d-dd1fcb764eec",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "4a59577c-929d-47ff-990d-dd1fcb764eec"
      },
      "outputs": [],
      "source": [
        "microchips_data = \"\"\"0.051267,0.69956,1\n",
        "-0.092742,0.68494,1\n",
        "-0.21371,0.69225,1\n",
        "-0.375,0.50219,1\n",
        "-0.51325,0.46564,1\n",
        "-0.52477,0.2098,1\n",
        "-0.39804,0.034357,1\n",
        "-0.30588,-0.19225,1\n",
        "0.016705,-0.40424,1\n",
        "0.13191,-0.51389,1\n",
        "0.38537,-0.56506,1\n",
        "0.52938,-0.5212,1\n",
        "0.63882,-0.24342,1\n",
        "0.73675,-0.18494,1\n",
        "0.54666,0.48757,1\n",
        "0.322,0.5826,1\n",
        "0.16647,0.53874,1\n",
        "-0.046659,0.81652,1\n",
        "-0.17339,0.69956,1\n",
        "-0.47869,0.63377,1\n",
        "-0.60541,0.59722,1\n",
        "-0.62846,0.33406,1\n",
        "-0.59389,0.005117,1\n",
        "-0.42108,-0.27266,1\n",
        "-0.11578,-0.39693,1\n",
        "0.20104,-0.60161,1\n",
        "0.46601,-0.53582,1\n",
        "0.67339,-0.53582,1\n",
        "-0.13882,0.54605,1\n",
        "-0.29435,0.77997,1\n",
        "-0.26555,0.96272,1\n",
        "-0.16187,0.8019,1\n",
        "-0.17339,0.64839,1\n",
        "-0.28283,0.47295,1\n",
        "-0.36348,0.31213,1\n",
        "-0.30012,0.027047,1\n",
        "-0.23675,-0.21418,1\n",
        "-0.06394,-0.18494,1\n",
        "0.062788,-0.16301,1\n",
        "0.22984,-0.41155,1\n",
        "0.2932,-0.2288,1\n",
        "0.48329,-0.18494,1\n",
        "0.64459,-0.14108,1\n",
        "0.46025,0.012427,1\n",
        "0.6273,0.15863,1\n",
        "0.57546,0.26827,1\n",
        "0.72523,0.44371,1\n",
        "0.22408,0.52412,1\n",
        "0.44297,0.67032,1\n",
        "0.322,0.69225,1\n",
        "0.13767,0.57529,1\n",
        "-0.0063364,0.39985,1\n",
        "-0.092742,0.55336,1\n",
        "-0.20795,0.35599,1\n",
        "-0.20795,0.17325,1\n",
        "-0.43836,0.21711,1\n",
        "-0.21947,-0.016813,1\n",
        "-0.13882,-0.27266,1\n",
        "0.18376,0.93348,0\n",
        "0.22408,0.77997,0\n",
        "0.29896,0.61915,0\n",
        "0.50634,0.75804,0\n",
        "0.61578,0.7288,0\n",
        "0.60426,0.59722,0\n",
        "0.76555,0.50219,0\n",
        "0.92684,0.3633,0\n",
        "0.82316,0.27558,0\n",
        "0.96141,0.085526,0\n",
        "0.93836,0.012427,0\n",
        "0.86348,-0.082602,0\n",
        "0.89804,-0.20687,0\n",
        "0.85196,-0.36769,0\n",
        "0.82892,-0.5212,0\n",
        "0.79435,-0.55775,0\n",
        "0.59274,-0.7405,0\n",
        "0.51786,-0.5943,0\n",
        "0.46601,-0.41886,0\n",
        "0.35081,-0.57968,0\n",
        "0.28744,-0.76974,0\n",
        "0.085829,-0.75512,0\n",
        "0.14919,-0.57968,0\n",
        "-0.13306,-0.4481,0\n",
        "-0.40956,-0.41155,0\n",
        "-0.39228,-0.25804,0\n",
        "-0.74366,-0.25804,0\n",
        "-0.69758,0.041667,0\n",
        "-0.75518,0.2902,0\n",
        "-0.69758,0.68494,0\n",
        "-0.4038,0.70687,0\n",
        "-0.38076,0.91886,0\n",
        "-0.50749,0.90424,0\n",
        "-0.54781,0.70687,0\n",
        "0.10311,0.77997,0\n",
        "0.057028,0.91886,0\n",
        "-0.10426,0.99196,0\n",
        "-0.081221,1.1089,0\n",
        "0.28744,1.087,0\n",
        "0.39689,0.82383,0\n",
        "0.63882,0.88962,0\n",
        "0.82316,0.66301,0\n",
        "0.67339,0.64108,0\n",
        "1.0709,0.10015,0\n",
        "-0.046659,-0.57968,0\n",
        "-0.23675,-0.63816,0\n",
        "-0.15035,-0.36769,0\n",
        "-0.49021,-0.3019,0\n",
        "-0.46717,-0.13377,0\n",
        "-0.28859,-0.060673,0\n",
        "-0.61118,-0.067982,0\n",
        "-0.66302,-0.21418,0\n",
        "-0.59965,-0.41886,0\n",
        "-0.72638,-0.082602,0\n",
        "-0.83007,0.31213,0\n",
        "-0.72062,0.53874,0\n",
        "-0.59389,0.49488,0\n",
        "-0.48445,0.99927,0\n",
        "-0.0063364,0.99927,0\n",
        "0.63265,-0.030612,0\"\"\"\n",
        "\n",
        "with open('./data/microchips.csv', 'w') as f:\n",
        "    f.write(microchips_data)\n",
        "\n",
        "# Importando dados\n",
        "X_features = ['Microchip Test 1', 'Microchip Test 2']\n",
        "y_name = ['Pass']\n",
        "data = pd.read_csv(\"./data/microchips.csv\", header=None, names=X_features + y_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bca491b3-5755-4fdd-898d-d8ad049f3bfc",
      "metadata": {
        "id": "bca491b3-5755-4fdd-898d-d8ad049f3bfc"
      },
      "source": [
        "### 4.a) Crie a matriz de treino $X_{\\rm train}$, o vetor de variáveis alvo $y_{\\rm train}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ac0a1b-31eb-428e-86ef-b245fcb8bf5f",
      "metadata": {
        "id": "a6ac0a1b-31eb-428e-86ef-b245fcb8bf5f"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe31470-9f57-443c-a9a1-63b1522d36ad",
      "metadata": {
        "id": "bfe31470-9f57-443c-a9a1-63b1522d36ad"
      },
      "source": [
        "### 4b) Explore (rapidamente) os dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73260d18-4644-4363-af85-370a33201fd5",
      "metadata": {
        "id": "73260d18-4644-4363-af85-370a33201fd5"
      },
      "source": [
        "**Inspecione a tabela com funcionalidade da biblioteca pandas e faça um scatterplot dos dados, usando a coluna \"Admitted\" como estilo do marcador (cor, forma ou ambos).**\n",
        "\n",
        "_Obs: não gaste muito tempo neste exercício. Por outro lado, é sempre importante inspecionar rapidamente os dados._\n",
        "\n",
        "\n",
        "> Besides `display()`, you can use several other methods to inspect a pandas DataFrame:\n",
        ">\n",
        "> - `.head()`: Shows the first 5 rows (or a specified number) of the DataFrame. Useful for a quick look at the data structure.\n",
        "> - `.info()`: Prints a concise summary of the DataFrame, including the index dtype > and column dtypes, non-null values and memory usage.\n",
        "> - `.describe()`: Generates descriptive statistics of the DataFrame's numerical > columns, such as count, mean, standard deviation, minimum, and maximum.\n",
        "> - `.shape`: Returns a tuple representing the dimensionality of the DataFrame (rows, > columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7607146f-524e-4182-888b-bc4cdf9d8afb",
      "metadata": {
        "id": "7607146f-524e-4182-888b-bc4cdf9d8afb"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74377f5c-8b99-40e2-b8f9-2c2a70e2089a",
      "metadata": {
        "id": "74377f5c-8b99-40e2-b8f9-2c2a70e2089a"
      },
      "source": [
        "### 4.c) Crie um conjunto de features polinomiais até ordem 6, contendo todos os dados cruzados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93870b71-af8e-4595-8d3b-62d126312e67",
      "metadata": {
        "id": "93870b71-af8e-4595-8d3b-62d126312e67"
      },
      "source": [
        "_Obs: Use o método `PolynomialFeatures` da biblioteca Scikit-learn. Use a opção `include_bias=False` para não introduzir uma feature constante._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638cd7ef-8060-42e1-b14f-1c539e778228",
      "metadata": {
        "id": "638cd7ef-8060-42e1-b14f-1c539e778228"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# X_poly_train = ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7f9fa6-3a23-476a-917d-ecdf095dc5f6",
      "metadata": {
        "id": "cc7f9fa6-3a23-476a-917d-ecdf095dc5f6",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 5: Regressão logística com regularização - Método do gradiente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8dd2bf-c5e8-4506-a3ed-c4c2beb8dfd0",
      "metadata": {
        "id": "8c8dd2bf-c5e8-4506-a3ed-c4c2beb8dfd0"
      },
      "source": [
        "A nova matriz de dados `X_poly_train` possui 27 features por linha, estando bem suscetível ao problema de _overfitting_. Neste exercício, vamos adaptar e rodar o método do gradiente para o caso com regularização. Ao longo do exercício, será possível reutilizar as funções definidas no exercício 2 e incluir a regularização adicionalmente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea00d35a-19ea-4d7f-b644-ebf5f7d9b56e",
      "metadata": {
        "id": "ea00d35a-19ea-4d7f-b644-ebf5f7d9b56e"
      },
      "source": [
        "### 5.a) Implementando as funções"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7468b1-4844-4876-8822-7eca1d8e53a3",
      "metadata": {
        "id": "fd7468b1-4844-4876-8822-7eca1d8e53a3"
      },
      "source": [
        "Será necessário definir as seguintes funções:\n",
        "\n",
        "- `compute_cost_reg` implementa a equação (3) acima;\n",
        "- `compute_gradient_reg` implementa as equações (6a) e (6b) acima;\n",
        "- `gradient_descent` é idêntico ao método desenvolvido no Exercício 2, exceto pela modificação das chamadas às duas funções acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd85a1c-473d-45e6-aaae-f81f5de163ce",
      "metadata": {
        "id": "bbd85a1c-473d-45e6-aaae-f81f5de163ce",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples.\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar, float) Controls amount of regularization\n",
        "\n",
        "    Returns:\n",
        "      total_cost : (scalar)     cost\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    cost = 0\n",
        "    m = X.shape[0]\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i], w) + b)\n",
        "        cost += -y[i]*np.log(f_wb_i) - (1 - y[i])*np.log(1 - f_wb_i)\n",
        "\n",
        "    cost =\n",
        "    # ----\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n",
        "def compute_gradient_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression with regularization\n",
        "\n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
        "      y : (ndarray Shape (m,))  target value\n",
        "      w : (ndarray Shape (n,))  values of parameters of the model\n",
        "      b : (scalar)              value of bias parameter of the model\n",
        "      lambda_ : (scalar,float)  regularization constant\n",
        "    Returns\n",
        "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b.\n",
        "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    # ...\n",
        "    # ----\n",
        "\n",
        "    return dj_dw, dj_db\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, lambda_):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m, n)) : Data, m examples with n features each\n",
        "      y (ndarray (m,))   : target values\n",
        "      w_in (ndarray (n,)): initial model weight parameters\n",
        "      b_in (scalar)      : initial model bias parameter\n",
        "      alpha (float)      : Learning rate\n",
        "      num_iters (int)    : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,))                   : Updated values of parameters\n",
        "      b (scalar)                         : Updated value of parameter\n",
        "      J_history (ndarray (num_iters,))   : History of cost values\n",
        "      w_history (ndarray (num_iters, n)) : History of parameters w\n",
        "      b_history (ndarray (num_iters,))   : History of parameters b\n",
        "      \"\"\"\n",
        "\n",
        "    # ---- INSIRA SEU CÓDIGO AQUI\n",
        "    # ...\n",
        "    # ----\n",
        "\n",
        "    return w, b, J_history, w_history, b_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a217ae98-9e46-4bc8-8d96-c81e78269480",
      "metadata": {
        "id": "a217ae98-9e46-4bc8-8d96-c81e78269480"
      },
      "source": [
        "### 5.b) Treine o modelo, faça previsões e inspecione graficamente os resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63465684-e1ab-4594-93f7-f1abbc36c8ee",
      "metadata": {
        "id": "63465684-e1ab-4594-93f7-f1abbc36c8ee"
      },
      "source": [
        "Faça a regressão logística nos dados e verifique graficamente que o comportamento da função de custo está como o esperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce44635-7ab3-4446-ba3e-c2b063c9ea96",
      "metadata": {
        "id": "cce44635-7ab3-4446-ba3e-c2b063c9ea96"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ffa69da-7ce1-4dd2-a658-2019605e272b",
      "metadata": {
        "id": "2ffa69da-7ce1-4dd2-a658-2019605e272b",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercício 6: Regressão logística com regularização - Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00bf1da8-3bbf-49f1-91c5-eb15487902c1",
      "metadata": {
        "id": "00bf1da8-3bbf-49f1-91c5-eb15487902c1"
      },
      "source": [
        "O objetivo deste exercício é obter os mesmos resultados do exercício acima usando a funcionalidade já existente no scikit-learn. Para tanto, usaremos novamente a função `PolynomialFeatures` para processar os dados de entrada `X_train`, seguida da função `LogisticRegression` para realizar o treinamento.\n",
        "\n",
        "Para combinar as operações, usaremos a função `Pipeline` que, como indica o nome, cria uma sequência de operações (i.e. um _pipeline_) definidas pelo usuário.\n",
        "\n",
        "_PS. Como as features já estão escalonadas, não usaremos o `StandardScaler` para renormalizar os dados._\n",
        "\n",
        "_PPS. Como criaremos novamente as features polinomiais, use `X_train` e não `X_poly_train` quando realizar o treinamento no ex 6.b)_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f4f5ee-431a-406f-ad50-982076285a85",
      "metadata": {
        "id": "01f4f5ee-431a-406f-ad50-982076285a85"
      },
      "source": [
        "### 6.a) Defina um pipeline em scikit-learn que crie features polinomiais e defina um modelo de regressão logística com regularização 'ridge'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d5b821-35d5-4abe-a5ef-7bc312f5e0e6",
      "metadata": {
        "id": "a4d5b821-35d5-4abe-a5ef-7bc312f5e0e6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0698c34-69f3-4c20-b545-60bd2b0b59f6",
      "metadata": {
        "id": "f0698c34-69f3-4c20-b545-60bd2b0b59f6"
      },
      "source": [
        "### 6.b) Treine o modelo, faça previsões e inspecione graficamente os resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712a9383-2e20-4c3e-91eb-3011638611c9",
      "metadata": {
        "id": "712a9383-2e20-4c3e-91eb-3011638611c9"
      },
      "outputs": [],
      "source": [
        "# ---- INSIRA SEU CÓDIGO AQUI\n",
        "# ...\n",
        "# ----"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}